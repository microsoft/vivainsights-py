{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5f1358",
   "metadata": {},
   "source": [
    "# Copilot X Glint Sentiment Relationship Strength\n",
    "This Jupyter Notebook analyzes employee engagement data, focusing on \"Copilot Usage\" metrics. \n",
    "The code performs the following tasks:\n",
    "1. Imports and cleans data from a CSV file that includes both Glint Score and Copilot Usage by user (Viva Insights Export).\n",
    "2. Converts engagement scores to a 100-point scale and categorizes them into favorability groups (droping NEU responses).\n",
    "3. Calculates weekly Copilot usage for employees.\n",
    "4. Categorizes employees into user groups (e.g., PowerUser, NoviceUser) based on their Copilot activity.\n",
    "5. Analyzes the relationship between Copilot usage categories and engagement scores using odds ratios.\n",
    "6. Outputs insights on the strongest relationships between user categories and engagement metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a6225",
   "metadata": {},
   "source": [
    "### Install Libraries and initialization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9192a824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Encoding: ascii\n"
     ]
    }
   ],
   "source": [
    "# pip install --user pandas\n",
    "#pip install --user scipy\n",
    "# file_name: Path to the CSV file containing the data.\n",
    "# item_options: Number of options in the survey (default is 5 for a 5-point scale).\n",
    "# survey_close_date: The date when the survey closed (format: YYYY-MM-DD).\n",
    "\n",
    "import chardet\n",
    "import re\n",
    "from scipy.stats import percentileofscore\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "survey_close_date = datetime.datetime.strptime('2024-09-29', '%Y-%m-%d')\n",
    "file_name = 'C:/Users/bentankus/OneDrive - Microsoft/Projects/Copilot Support/glint_demodata_singlesource.csv'\n",
    "item_options = 5 # Number of options in the survey (default is 5 for a 5-point scale).\n",
    "\n",
    "\n",
    "######## IMPORT DATA  ########\n",
    "# The code will consider any data with \"Copilot\" in the column name as copilot activities.\n",
    "\n",
    "# EXPECTED DATA STRUCTURE\n",
    "## | EMPLOYEE IDENTIFIER | GLINT ITEM | GLINT ITEM SCORE | COPILOT ITEMS ... | DATE OF COPILOT WEEKLY USAGE | \n",
    "\n",
    "# SINGLE DATA SOURCE\n",
    "# SENSE DATA ENCODING\n",
    "with open(file_name, \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(f\"Detected Encoding: {result['encoding']}\")\n",
    "# Load the Excel file into a pandas DataFrame\n",
    "df_glint = pd.read_csv(file_name , encoding=result['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e51d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CLEAN DATA  ########\n",
    "# (Convert to favorability and drop neutral responses)\n",
    "\n",
    "# CONVERT TO 100PT SCALE FOR EASY FAVORABILITY PARSING\n",
    "df_glint['score100'] = (df_glint['Score'] - 1) * (100/(item_options-1))\n",
    "\n",
    "# CONVERT TO FAVORABILITY\n",
    "df_glint['favorability'] = df_glint['score100'].apply(lambda x: 'fav' if x > 70 else ('unfav' if x < 40 else 'neu'))\n",
    "\n",
    "# CREATE FULL DF FOR LATER USE\n",
    "df_glint_full = df_glint\n",
    "\n",
    "# DROP NEUTRAL FAVORABILITY\n",
    "df_glint = df_glint[df_glint['favorability'] != 'neu']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda2878",
   "metadata": {},
   "source": [
    "### Analyse Data\n",
    "\n",
    "Calculate \"Copilot Usage\" based on 12 week habit building literature and Odds Ratio methods comparing differing user types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603b7594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bentankus\\AppData\\Local\\Temp\\ipykernel_9128\\1602427854.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_past_12_weeks['NonZero_Copilot_Activities'] = df_past_12_weeks.iloc[:, 2:].apply(lambda row: (row > 0).sum(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "#Calculate \"Copilot Usage\" based on 12 week habit building literature and Odds Ratio methods comparing differing user types\n",
    "\n",
    "\n",
    "# CALCULATE COPILOT USAGE\n",
    "columns_to_include = ['Employee_ID', 'MetricDate']\n",
    "# Dynamically filter columns based on conditions\n",
    "copilot_columns = df_glint.filter(\n",
    "    regex='(?i)copilot', axis=1\n",
    ").columns\n",
    "\n",
    "# Additional filtering logic can be added here\n",
    "# Example: Exclude columns containing specific keywords\n",
    "exclude_keywords = ['Test', 'day']\n",
    "copilot_columns = copilot_columns[~copilot_columns.str.contains('|'.join(exclude_keywords), case=False)]\n",
    "\n",
    "# Select employee_id, date, and copilot-related columns\n",
    "df_glintscore_copilot = df_glint[columns_to_include + copilot_columns.tolist()].drop_duplicates()\n",
    "df_glintscore_copilot['MetricDate'] = pd.to_datetime(df_glintscore_copilot['MetricDate'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Define the start date for the past 12 weeks based on the survey close date\n",
    "start_date = survey_close_date - datetime.timedelta(weeks=12)\n",
    "\n",
    "# Filter data for the past 12 weeks\n",
    "df_past_12_weeks = df_glintscore_copilot[\n",
    "    (df_glintscore_copilot['MetricDate'] >= start_date) & \n",
    "    (df_glintscore_copilot['MetricDate'] < survey_close_date)\n",
    "]\n",
    "\n",
    "# COUNT ALL NON-ZERO COPILOT ACTIVITIES FOR EACH EMPLOYEE_ID AND DATE\n",
    "df_past_12_weeks['NonZero_Copilot_Activities'] = df_past_12_weeks.iloc[:, 2:].apply(lambda row: (row > 0).sum(), axis=1)\n",
    "\n",
    "# Group by Employee_ID and calculate weekly usage\n",
    "weekly_usage = df_past_12_weeks.groupby('Employee_ID')['NonZero_Copilot_Activities'].agg(\n",
    "    total_usage='sum',\n",
    "    avg_usage='mean'\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# JOIN IN df_past_12_weeks_usage NonZero_Copilot_Activities FOR EACH USER TO weekly_usage SO WE HAVE HABIT BUILDING\n",
    "weekly_usage = weekly_usage.merge(df_past_12_weeks[['Employee_ID', 'NonZero_Copilot_Activities']].drop_duplicates(), on='Employee_ID', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67962631",
   "metadata": {},
   "source": [
    "### Assign user categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df52410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Count  Percent\n",
      "UserCategory_Group                \n",
      "NoviceUser            272    98.55\n",
      "HabitualUser            4     1.45\n",
      "\n",
      "WARNING: HabitualUser is below 5% with 4.0 users (1.45%)\n",
      "Consider redefining user categories to ensure each group has a minimum of 5% of the total user base.\n"
     ]
    }
   ],
   "source": [
    "# Assign user categories\n",
    "# Power User: averaging 15+ weekly total Copilot actions and any use of Copilot in at least 9 out of past 12 weeks.\n",
    "# Habitual User: any use of Copilot in at least 9 out of past 12 weeks.\n",
    "# Novice User: averaging at least one Copilot action over the last 12 weeks.\n",
    "# Low User: having any Copilot action in the past 12 weeks.\n",
    "# Non-user: zero Copilot actions in the last 12 weeks.\n",
    "def assign_user_category(row):\n",
    "    if row['total_usage'] > 0:\n",
    "        if row['NonZero_Copilot_Activities'] >= 9:\n",
    "            if row['avg_usage'] >= 15:\n",
    "                return 'PowerUser'\n",
    "            else:\n",
    "                return 'HabitualUser'\n",
    "    \n",
    "        elif row['avg_usage'] >= 1:\n",
    "            return 'NoviceUser'\n",
    "        \n",
    "        elif row['total_usage'] == 1:\n",
    "            return 'LowUser'\n",
    "\n",
    "        else:\n",
    "            return 'NonUser'\n",
    "    \n",
    "    return \n",
    "\n",
    "weekly_usage['UserCategory'] = weekly_usage.apply(assign_user_category, axis=1)\n",
    "\n",
    "# RENAME COLUMN\n",
    "weekly_usage['UserCategory_Group'] = weekly_usage['UserCategory']\n",
    "\n",
    "# Merge the user categories back into the original DataFrame\n",
    "df_past_12_weeks_usage = df_past_12_weeks.merge(\n",
    "    weekly_usage[['Employee_ID', 'UserCategory_Group']],\n",
    "    on='Employee_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# MERGE USERCATEGORY BACK INTO GLINT DATA\n",
    "df_glint_usage = df_glint.merge(\n",
    "    df_past_12_weeks_usage[['Employee_ID', 'UserCategory_Group']],\n",
    "    on='Employee_ID',\n",
    "    how='left'\n",
    ") [['Employee_ID', 'Glint_Item', 'Score', 'UserCategory_Group']]\n",
    "\n",
    "\n",
    "# OUTPUT THE COUNT OF EACH USER CATEGORY AND PERCENT OF TOTAL FORMATTED IN A TABLE\n",
    "user_category_counts = weekly_usage['UserCategory_Group'].value_counts()\n",
    "user_category_percent = user_category_counts / user_category_counts.sum() * 100\n",
    "user_category_table = pd.concat([user_category_counts, user_category_percent], axis=1)\n",
    "user_category_table.columns = ['Count', 'Percent']\n",
    "user_category_table = user_category_table.round(2)\n",
    "print(user_category_table)\n",
    "\n",
    "\n",
    "# ADD A USER CATEGORY FLAG ALERT THAT READS user_category_table AND PRINTS ALERT IF ANY PERCENT IS LESS THAN 5%\n",
    "def check_user_category_alerts(user_category_table):\n",
    "    for category, row in user_category_table.iterrows():\n",
    "        if row['Percent'] < 5:\n",
    "            print('')\n",
    "            print(f\"WARNING: {category} is below 5% with {row['Count']} users ({row['Percent']:.2f}%)\")\n",
    "            print(\"Consider redefining user categories to ensure each group has a minimum of 5% of the total user base.\")\n",
    "\n",
    "check_user_category_alerts(user_category_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645c902",
   "metadata": {},
   "source": [
    "### Convert to Favorability and begin the Odds Ratio calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750b6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD FAVORABILITY COLUMN\n",
    "df_glint_usage['favorability'] = df_glint_usage['Score'].apply(lambda x: 'fav' if x >= 4 else ('unfav' if x <= 2 else 'neu'))\n",
    "df_glint_usage = df_glint_usage[df_glint_usage['favorability'] != 'neu']\n",
    "\n",
    "# GET COUNTS FOR EACH USER CATEGORY GROUPED BY GLINT ITEM AND FAVORABILITY\n",
    "df_glint_usage_grouped = df_glint_usage.groupby(['Glint_Item', 'favorability', 'UserCategory_Group'])['Employee_ID'].nunique().reset_index(name='user_count')\n",
    "\n",
    "# ADD 0.5 TO EACH COUNT TO AVOID DIVISION BY ZERO AND INF ODDS\n",
    "df_glint_usage_grouped['user_count'] = df_glint_usage_grouped['user_count'] + 0.5\n",
    "\n",
    "# PIVOT THE DATAFRAME TO GET FAV / UNFAV IN COLUMNS\n",
    "df_glint_usage_grouped_pivot = df_glint_usage_grouped.pivot_table(index=['Glint_Item', 'UserCategory_Group'], columns='favorability', values='user_count').reset_index()\n",
    "\n",
    "# DIVIDE FAV BY UNFAV TO GET ODDS\n",
    "df_glint_usage_grouped_pivot['odds'] = df_glint_usage_grouped_pivot['fav'] / df_glint_usage_grouped_pivot['unfav']\n",
    "\n",
    "# DROP NAN ODDS, THEN ADD \"MAX\" OR \"MIN\" LABEL TO THE THE MAX-MIN ODDS FOR EACH GLINT ITEM\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot.dropna(subset=['odds'])\n",
    "\n",
    "# DROP FAV AND UNFAV COLUMN AND PIVOT OUT USERCATEGORY_GROUP VALUES INTO NEW COLUMNS\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.drop(columns=['fav', 'unfav'])\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.pivot(index='Glint_Item', columns='UserCategory_Group', values='odds').reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a35606",
   "metadata": {},
   "source": [
    "### Calculate and display final odds ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526e390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Glint_Item  odds_ratio       UserCategory_Group\n",
      "0  Q_ESAT_HAPPY    1.320000  HabitualUser-NoviceUser\n",
      "1  Q_JOB_GROWTH    1.248062  HabitualUser-NoviceUser\n"
     ]
    }
   ],
   "source": [
    "# DROP NAN ODDS, THEN ADD \"MAX\" OR \"MIN\" LABEL TO THE THE MAX-MIN ODDS FOR EACH GLINT ITEM\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot.dropna(subset=['odds'])\n",
    "\n",
    "# DROP FAV AND UNFAV COLUMNS \n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.drop(columns=['fav', 'unfav'])\n",
    "\n",
    "# Identify the MAX and MIN odds for each Glint_Item and UserCategory_Group\n",
    "df_glint_usage_grouped_pivot_odds['MAX_MIN'] = df_glint_usage_grouped_pivot_odds.groupby('Glint_Item')['odds'].transform(\n",
    "    lambda x: ['MAX' if val == x.max() else 'MIN' if val == x.min() else None for val in x])\n",
    "\n",
    "# DROP NONE VALUES FROM MAX_MIN COLUMN\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.dropna(subset=['MAX_MIN'])\n",
    "\n",
    "# PREPARE df_max_min FOR LATER JOIN\n",
    "df_max_min = df_glint_usage_grouped_pivot_odds[['Glint_Item', 'UserCategory_Group','MAX_MIN']]\n",
    "\n",
    "# DROP USERCATEGORY_GROUP COLUMN AND PIVOT MAX_MIN INTO COLUMNS\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.drop(columns='UserCategory_Group').drop_duplicates()\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.pivot(index='Glint_Item', columns='MAX_MIN', values='odds').reset_index()\n",
    "\n",
    "# TRY TO DIVIDE THE MAX ODDS BY THE MIN ODDS FOR EACH GLINT ITEM. IF THERE IS NO MIN COLUMN, PRINT ('NOT ENOUGH DATA')\n",
    "try:\n",
    "    df_glint_usage_grouped_pivot_odds['odds_ratio'] = df_glint_usage_grouped_pivot_odds['MAX'] / df_glint_usage_grouped_pivot_odds['MIN']\n",
    "except KeyError:\n",
    "    print('NOT ENOUGH DATA')\n",
    "    df_glint_usage_grouped_pivot_odds['odds_ratio'] = None\n",
    "\n",
    "# DROP ANY ROW FROM df_glint_usage_grouped_pivot_odds THAT HAS A NAN VALUE\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.dropna()\n",
    "\n",
    "# JOIN df_glint_usage_grouped_pivot_odds WITH df_max_min TO GET THE USERCATEGORY_GROUP FOR THE MAX ODDS\n",
    "df_glint_usage_grouped_pivot_odds = df_glint_usage_grouped_pivot_odds.merge(df_max_min, on='Glint_Item', how='left')\n",
    "\n",
    "# Group by 'Glint_Item' and 'odds_ratio', then concatenate 'UserCategory_Group' values into a single string\n",
    "df_glint_usage_grouped_pivot_odds_final = df_glint_usage_grouped_pivot_odds.groupby(['Glint_Item', 'odds_ratio']).agg({\n",
    "    'UserCategory_Group': lambda x: '-'.join(x).replace(' ', '')\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PRINT FINAL ODDS RATIO TABLE SORTED BY ODDS RATIO\n",
    "df_glint_usage_grouped_pivot_odds_final = df_glint_usage_grouped_pivot_odds_final.sort_values('odds_ratio', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NEW LINE FOR READABILITY\n",
    "print('')\n",
    "\n",
    "print(df_glint_usage_grouped_pivot_odds_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa80258",
   "metadata": {},
   "source": [
    "### Calculate Glint Score for each user group and item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a5ee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glint_Item</th>\n",
       "      <th>UserCategory_Group</th>\n",
       "      <th>score100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q_BELONGING</td>\n",
       "      <td>HabitualUser</td>\n",
       "      <td>56.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q_BELONGING</td>\n",
       "      <td>NoviceUser</td>\n",
       "      <td>54.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q_CULTURE_RECOGNITION</td>\n",
       "      <td>HabitualUser</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q_CULTURE_RECOGNITION</td>\n",
       "      <td>NoviceUser</td>\n",
       "      <td>56.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q_ESAT_HAPPY</td>\n",
       "      <td>HabitualUser</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q_ESAT_HAPPY</td>\n",
       "      <td>NoviceUser</td>\n",
       "      <td>56.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q_ESAT_RECOMMEND</td>\n",
       "      <td>HabitualUser</td>\n",
       "      <td>81.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q_ESAT_RECOMMEND</td>\n",
       "      <td>NoviceUser</td>\n",
       "      <td>50.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q_JOB_GROWTH</td>\n",
       "      <td>HabitualUser</td>\n",
       "      <td>68.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q_JOB_GROWTH</td>\n",
       "      <td>NoviceUser</td>\n",
       "      <td>57.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Glint_Item UserCategory_Group  score100\n",
       "0            Q_BELONGING       HabitualUser      56.2\n",
       "1            Q_BELONGING         NoviceUser      54.5\n",
       "2  Q_CULTURE_RECOGNITION       HabitualUser      75.0\n",
       "3  Q_CULTURE_RECOGNITION         NoviceUser      56.4\n",
       "4           Q_ESAT_HAPPY       HabitualUser      50.0\n",
       "5           Q_ESAT_HAPPY         NoviceUser      56.1\n",
       "6       Q_ESAT_RECOMMEND       HabitualUser      81.2\n",
       "7       Q_ESAT_RECOMMEND         NoviceUser      50.7\n",
       "8           Q_JOB_GROWTH       HabitualUser      68.8\n",
       "9           Q_JOB_GROWTH         NoviceUser      57.7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glint_agg = df_glint_full[['Employee_ID','Glint_Item','score100']].drop_duplicates()\n",
    "\n",
    "\n",
    "# JOIN IN USERCATEGORY GROUP TO df_glint_agg\n",
    "df_glint_agg = df_glint_agg.merge(\n",
    "    df_glint_usage[['Employee_ID', 'UserCategory_Group']].drop_duplicates(),\n",
    "    on='Employee_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# GROUPED BY GLINT ITEM AND USERCATEGORY GROUP AND CALCULATE THE MEAN SCORE FROM SCORE100\n",
    "df_glint_agg = round(df_glint_agg.groupby(['Glint_Item', 'UserCategory_Group'])['score100'].mean().reset_index(),1)\n",
    "\n",
    "df_glint_agg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
